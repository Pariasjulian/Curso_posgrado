{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Reinforcement Learning / Tensorflow - TF_Agents \n\nEl aprendizaje por refuerzo (RL) es uno de los campos más antiguos del aprendizaje automático. Ha existido desde la década de 1950 y ha producido muchas aplicaciones interesantes a lo largo de los años.\n\n<br />\n<img src='https://es.mathworks.com/help///reinforcement-learning/ug/reinforcement_learning_diagram.png' width='300' />\n\n*\"El aprendizaje por refuerzo se diferencia del aprendizaje supervisado en que no requiere la presentación de pares de entrada/salida etiquetados y no requiere que se corrijan explícitamente acciones subóptimas. En cambio, la atención se centra en encontrar un equilibrio entre la exploración (de territorio desconocido) y la explotación (del conocimiento actual)..\"* [wikipedia](https://en.wikipedia.org/wiki/Reinforcement_learning)","metadata":{}},{"cell_type":"code","source":"#librerias necesarias\n!sudo apt-get update\n!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n!pip install 'imageio==2.4.0'\n!pip install pyvirtualdisplay\n!pip install tf-agents[reverb]\n!pip install pyglet\n!pip install swig\n!pip install gym[atari,box2d,accept-rom-license]  #install gym and virtual display","metadata":{"id":"KEHR2Ui-lo8O","execution":{"iopub.status.busy":"2023-11-30T05:14:40.378989Z","iopub.execute_input":"2023-11-30T05:14:40.379267Z","iopub.status.idle":"2023-11-30T05:18:59.839194Z","shell.execute_reply.started":"2023-11-30T05:14:40.379242Z","shell.execute_reply":"2023-11-30T05:18:59.838115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from __future__ import absolute_import, division, print_function\n\nimport base64\nimport imageio\nimport IPython\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport PIL.Image\nimport pyvirtualdisplay\nimport reverb\n\nimport tensorflow as tf\n\nfrom tf_agents.agents.dqn import dqn_agent\nfrom tf_agents.drivers import py_driver\nfrom tf_agents.environments import suite_gym\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.eval import metric_utils\nfrom tf_agents.metrics import tf_metrics\nfrom tf_agents.networks import sequential\nfrom tf_agents.policies import py_tf_eager_policy\nfrom tf_agents.policies import random_tf_policy\nfrom tf_agents.replay_buffers import reverb_replay_buffer\nfrom tf_agents.replay_buffers import reverb_utils\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.utils import common\n\nfrom tf_agents.environments import  suite_gym\nfrom tf_agents.environments.atari_preprocessing import AtariPreprocessing\nfrom tf_agents.environments.atari_wrappers import FrameStack4\nimport gym\n\n# To get smooth animations\nimport matplotlib.animation as animation\nmatplotlib.rc('animation', html='jshtml')","metadata":{"id":"sMitx5qSgJk1","execution":{"iopub.status.busy":"2023-11-30T05:18:59.841329Z","iopub.execute_input":"2023-11-30T05:18:59.841801Z","iopub.status.idle":"2023-11-30T05:19:08.758164Z","shell.execute_reply.started":"2023-11-30T05:18:59.841764Z","shell.execute_reply":"2023-11-30T05:19:08.757174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setup...\n**Aprendendiendo a jugar River Raid** <br />\n<img src='https://www.gymlibrary.dev/_images/skiing.gif' width='300' />\n<br />\n*\"Skiing is a video game cartridge for the Atari 2600 console. It was authored by Bob Whitehead, and released by Activision in 1980.\n   Skiing is a single player only game, in which the player uses the joystick to control the direction and speed of a stationary skier at the top of the screen, while the background graphics scroll upwards, thus giving the illusion the skier is moving. The player must avoid obstacles, such as trees and moguls. The game cartridge was programmed with five variations each of two principal games. In the downhill mode, the player's goal is to reach the bottom of the ski course as rapidly as possible, while a timer records his relative success. In the slalom mode, the player must similarly reach the end of the course as rapidly as he can, but must at the same time pass through a series of gates (indicated by a pair of closely spaced flagpoles). Each gate missed counts as a penalty against the player's time.\"* [Wikipedia](https://en.wikipedia.org/wiki/Skiing_(Atari_2600_video_game))","metadata":{}},{"cell_type":"code","source":"#Carregando - River raid\nenv = suite_gym.load(environment_name=\"Skiing-v4\",max_episode_steps=27000, gym_env_wrappers=[AtariPreprocessing,FrameStack4])\nenv","metadata":{"execution":{"iopub.status.busy":"2023-11-30T05:19:08.759329Z","iopub.execute_input":"2023-11-30T05:19:08.759845Z","iopub.status.idle":"2023-11-30T05:19:08.995922Z","shell.execute_reply.started":"2023-11-30T05:19:08.759819Z","shell.execute_reply":"2023-11-30T05:19:08.994651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env.gym","metadata":{"execution":{"iopub.status.busy":"2023-11-30T05:19:08.998526Z","iopub.execute_input":"2023-11-30T05:19:08.998914Z","iopub.status.idle":"2023-11-30T05:19:09.006166Z","shell.execute_reply.started":"2023-11-30T05:19:08.998867Z","shell.execute_reply":"2023-11-30T05:19:09.004927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env.seed(42)\nenv.reset()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T05:19:09.007931Z","iopub.execute_input":"2023-11-30T05:19:09.008348Z","iopub.status.idle":"2023-11-30T05:19:09.254310Z","shell.execute_reply.started":"2023-11-30T05:19:09.008310Z","shell.execute_reply":"2023-11-30T05:19:09.253283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env.reset()\nimg = env.render(mode=\"rgb_array\")\nplt.figure(figsize=(4, 6))\nplt.imshow(img)\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T05:19:09.255557Z","iopub.execute_input":"2023-11-30T05:19:09.255852Z","iopub.status.idle":"2023-11-30T05:19:09.410505Z","shell.execute_reply.started":"2023-11-30T05:19:09.255827Z","shell.execute_reply":"2023-11-30T05:19:09.409079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set up a virtual display for rendering OpenAI gym environments.\ndisplay = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()","metadata":{"id":"J6HsdS5GbSjd","execution":{"iopub.status.busy":"2023-11-30T05:19:09.413065Z","iopub.execute_input":"2023-11-30T05:19:09.414126Z","iopub.status.idle":"2023-11-30T05:19:10.547357Z","shell.execute_reply.started":"2023-11-30T05:19:09.414075Z","shell.execute_reply":"2023-11-30T05:19:10.546500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.version.VERSION","metadata":{"id":"NspmzG4nP3b9","execution":{"iopub.status.busy":"2023-11-30T05:19:10.548553Z","iopub.execute_input":"2023-11-30T05:19:10.548901Z","iopub.status.idle":"2023-11-30T05:19:10.555095Z","shell.execute_reply.started":"2023-11-30T05:19:10.548848Z","shell.execute_reply":"2023-11-30T05:19:10.554137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Environment Specifications\nTF-Agents proporciona especificaciones para observaciones, acciones y pasos, incluidas sus respectivas formas.","metadata":{}},{"cell_type":"code","source":"print('Acciones disponibles:\\n{}\\r\\n'.format(env.gym.get_action_meanings()))\nprint('Observaciones:\\n{}'.format(env.observation_spec()))","metadata":{"execution":{"iopub.status.busy":"2023-11-30T05:19:10.556411Z","iopub.execute_input":"2023-11-30T05:19:10.556696Z","iopub.status.idle":"2023-11-30T05:19:10.568750Z","shell.execute_reply.started":"2023-11-30T05:19:10.556671Z","shell.execute_reply":"2023-11-30T05:19:10.568005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Environment Wrappers","metadata":{}},{"cell_type":"code","source":"#Aquí está la lista de wrappers disponibles:\nimport tf_agents.environments.wrappers\n\nfor name in dir(tf_agents.environments.wrappers):\n    obj = getattr(tf_agents.environments.wrappers, name)\n    if hasattr(obj, \"__base__\") and issubclass(obj, tf_agents.environments.wrappers.PyEnvironmentBaseWrapper):\n        print(\"{:27s} {}\".format(name, obj.__doc__.split(\"\\n\")[0]))","metadata":{"execution":{"iopub.status.busy":"2023-11-30T05:19:10.572845Z","iopub.execute_input":"2023-11-30T05:19:10.573177Z","iopub.status.idle":"2023-11-30T05:19:10.582393Z","shell.execute_reply.started":"2023-11-30T05:19:10.573150Z","shell.execute_reply":"2023-11-30T05:19:10.581424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Algunos ejemplos de acciones en el juego\nenv.reset()\ntime_step = env.step(np.array(0)) # FIRE\ntime_step = env.step(np.array(1)) # RIGHT\ntime_step = env.step(np.array(2)) # DOWNRIGHT\n\nobservation = time_step.observation.astype(np.float32)\n\n#Como existen 3 canales de colores, no podemos mostrar 4 frames.\nimage = observation[..., :3]\nimage = np.clip(image / 150, 0, 1)\nplt.imshow(image)\nplt.axis(\"off\")\nprint(observation.shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T05:19:10.583572Z","iopub.execute_input":"2023-11-30T05:19:10.583892Z","iopub.status.idle":"2023-11-30T05:19:10.690201Z","shell.execute_reply.started":"2023-11-30T05:19:10.583842Z","shell.execute_reply":"2023-11-30T05:19:10.689152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Para agrupar el entorno utilizamos TFPyEnviroment.","metadata":{}},{"cell_type":"code","source":"from tf_agents.environments.tf_py_environment import TFPyEnvironment\ntf_env = TFPyEnvironment(env)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T05:19:10.692259Z","iopub.execute_input":"2023-11-30T05:19:10.692557Z","iopub.status.idle":"2023-11-30T05:19:10.709559Z","shell.execute_reply.started":"2023-11-30T05:19:10.692523Z","shell.execute_reply":"2023-11-30T05:19:10.708638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DQN\n- TF-Agents proporciona algunos paquetes de red.\n\n- En este paquete, las imágenes se almacenan utilizando bytes del 0 al 255 para utilizar menos RAM.\n","metadata":{}},{"cell_type":"code","source":"from tf_agents.networks.q_network import QNetwork\n#convertir observacinoes a float float 32, normalizando.. (valores  0.0 a 1.0) \npreprocessing_layer = tf.keras.layers.Lambda( lambda obs: tf.cast(obs, np.float32) / 255.)\n\n#arquitectura:\n#conv_layer_params=[(32, (8, 8), 4), (64, (4, 4), 2), (64, (3, 3), 1)]\nconv_layer_params=[(32, (8, 8), 4) , (64, (4, 4), 2)]#, (64, (3, 3), 1), (1024, (7, 7), 1)]\n#layer dense com 512 por uma cama de sair de 4 unidades\nfc_layer_params=(1024,)\n\nq_network = QNetwork(tf_env.observation_spec(), tf_env.action_spec()\n                     ,preprocessing_layers=preprocessing_layer\n                     ,conv_layer_params=conv_layer_params\n                     ,fc_layer_params=fc_layer_params)\nq_network.summary","metadata":{"execution":{"iopub.status.busy":"2023-11-30T05:19:10.711028Z","iopub.execute_input":"2023-11-30T05:19:10.712036Z","iopub.status.idle":"2023-11-30T05:19:10.784822Z","shell.execute_reply.started":"2023-11-30T05:19:10.712006Z","shell.execute_reply":"2023-11-30T05:19:10.784124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DQN Agent\n[DQN paper ](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf)  ","metadata":{}},{"cell_type":"code","source":"from tf_agents.agents.dqn.dqn_agent import DqnAgent\n\ntrain_step = tf.Variable(0)\nupdate_period = 4 \n#optimizer = tf.keras.optimizers.Adam(lr=2.5e-4, rho=0.95, momentum=0.1,epsilon=0.00001, centered=True)\noptimizer = tf.compat.v1.train.RMSPropOptimizer(learning_rate=2.5e-4, decay=0.95, momentum=0.0,\n                                     epsilon=0.00001, centered=True)\n#optimizer = tf.keras.optimizers.Adam(lr=0.00005,epsilon=0.00001)\n#optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\nepsilon_fn = tf.keras.optimizers.schedules.PolynomialDecay(\n    initial_learning_rate=0.8,\n    decay_steps=250000 // update_period, \n    end_learning_rate=0.01)\n\nagent = DqnAgent(tf_env.time_step_spec(),\n                 tf_env.action_spec(),\n                 q_network=q_network,\n                 optimizer=optimizer,\n                 target_update_period=2000, \n                 #La función de pérdida debe devolver un error por instancia, por lo que definimos reducción=\"none\" \n                 td_errors_loss_fn=tf.keras.losses.Huber(reduction=\"none\"),\n                 gamma=0.89, # discount factor\n                 train_step_counter=train_step,\n                 epsilon_greedy=lambda: epsilon_fn(train_step))\nagent.initialize()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T05:19:10.786294Z","iopub.execute_input":"2023-11-30T05:19:10.786923Z","iopub.status.idle":"2023-11-30T05:19:23.304864Z","shell.execute_reply.started":"2023-11-30T05:19:10.786890Z","shell.execute_reply":"2023-11-30T05:19:23.303929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Se utiliza la [Huber loss](https://en.wikipedia.org/wiki/Huber_loss) como balance entre mse y mae","metadata":{}},{"cell_type":"markdown","source":"# Replay Buffer and the Corresponding Observer\n\n- La biblioteca TF-Agents proporciona algunas implementaciones de búfer de reproducción en el paquete tf_agents.replay_buffers.\n\n**max_length:**  1000000","metadata":{}},{"cell_type":"code","source":"from tf_agents.replay_buffers import tf_uniform_replay_buffer\n\n#data_spec datos que se guardarán en el búfer.\n#batch_size es el número de trayectorias que se deben agregar a cada paso.\n#max_length es la longitud máxima de reproducción. (Documento DQN2015: Cuidado con el acaparador de RAM)\n\nreplay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer( data_spec=agent.collect_data_spec,    batch_size=tf_env.batch_size, max_length=1000000)#ojo para el entrenamiento\n\nreplay_buffer_observer = replay_buffer.add_batch","metadata":{"execution":{"iopub.status.busy":"2023-11-30T05:19:23.306016Z","iopub.execute_input":"2023-11-30T05:19:23.306335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Metrics\n\nUsando las diversas métricas del paquete  tf_agents.metrics.","metadata":{}},{"cell_type":"code","source":"from tf_agents.metrics import tf_metrics\nfrom tf_agents.eval.metric_utils import log_metrics\nimport logging\n\ntrain_metrics = [\n    tf_metrics.NumberOfEpisodes(),\n    tf_metrics.EnvironmentSteps(),\n    tf_metrics.AverageReturnMetric(),\n    tf_metrics.AverageEpisodeLengthMetric(),\n]\nlogging.getLogger().setLevel(logging.INFO)\nlog_metrics(train_metrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Collect Driver\n\nUn collect driver es un objeto que explora un entorno mediante políticas, recoge experiencias de cada etapa y las transmite a los observadores.","metadata":{}},{"cell_type":"code","source":"from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n\ncollect_driver = DynamicStepDriver(\n    tf_env,\n    agent.collect_policy,\n    observers=[replay_buffer_observer] + train_metrics,\n    num_steps=update_period) \ncollect_driver","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tf_agents.policies.random_tf_policy import RandomTFPolicy\n\nclass ShowProgress:\n    def __init__(self, total):\n        self.counter = 0\n        self.total = total\n    def __call__(self, trajectory):\n        if not trajectory.is_boundary():\n            self.counter += 1\n        if self.counter % 100 == 0:\n            print(\"\\r{}/{}\".format(self.counter, self.total), end=\"\")\n\ninitial_collect_policy = RandomTFPolicy(tf_env.time_step_spec(),\n                                        tf_env.action_spec())\ninit_driver = DynamicStepDriver(\n    tf_env,\n    initial_collect_policy,\n    observers=[replay_buffer.add_batch, ShowProgress(20000)],\n    num_steps=20000)\nfinal_time_step, final_policy_state = init_driver.run()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Ejemplo de la trayectoria final de un episodio\ntrajectories, buffer_info = replay_buffer.get_next(sample_batch_size=2, num_steps=17)\ntrajectories, buffer_info, trajectories._fields","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tf_agents.trajectories.trajectory import to_transition\n\ntime_steps, action_steps, next_time_steps = to_transition(trajectories)\ntime_steps.observation.shape,trajectories.step_type.numpy()\n\n\nplt.figure(figsize=(10, 6.8))\nfor row in range(2):\n    for col in range(3):\n        plt.subplot(2, 3, row * 3 + col + 1)\n        obs = trajectories.observation[row, col].numpy().astype(np.float32)\n        img = obs[..., :3]\n        current_frame_delta = np.maximum(obs[..., 3] - obs[..., :3].mean(axis=-1), 0.)\n        img[..., 0] += current_frame_delta\n        img[..., 2] += current_frame_delta\n        img = np.clip(img / 150, 0, 1)\n        plt.imshow(img)\n        plt.axis(\"off\")\nplt.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0, wspace=0.02)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset\n\nConvertir los datos del búfer en un conjunto de datos para el entrenamiento","metadata":{}},{"cell_type":"code","source":"dataset = replay_buffer.as_dataset(\n    sample_batch_size=256,\n    num_steps=17,\n    num_parallel_calls=3\n).prefetch(3)\niterator = iter(dataset)\ntrajectories, buffer_info = next(iterator)\nplt.figure(figsize=(10, 6.8))\nfor row in range(2):\n    for col in range(3):\n        plt.subplot(2, 3, row * 3 + col + 1)\n        obs = trajectories.observation[row, col].numpy().astype(np.float32)\n        img = obs[..., :3]\n        current_frame_delta = np.maximum(obs[..., 3] - obs[..., :3].mean(axis=-1), 0.)\n        img[..., 0] += current_frame_delta\n        img[..., 2] += current_frame_delta\n        img = np.clip(img / 150, 0, 1)\n        plt.imshow(img)\n        plt.axis(\"off\")\nplt.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0, wspace=0.02)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = replay_buffer.as_dataset(\n    sample_batch_size=256,\n    num_steps=2,\n    num_parallel_calls=3).prefetch(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Loop","metadata":{}},{"cell_type":"code","source":"#convertir funciones\nfrom tf_agents.utils.common import function\n\ncollect_driver.run = function(collect_driver.run)\nagent.train = function(agent.train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_agent(n_iterations):\n    time_step = None\n    policy_state = agent.collect_policy.get_initial_state(tf_env.batch_size)\n    iterator = iter(dataset)\n    for iteration in range(n_iterations):\n        time_step, policy_state = collect_driver.run(time_step, policy_state)\n        trajectories, buffer_info = next(iterator)\n        train_loss = agent.train(trajectories)\n        print(\"\\r{} loss: {:.5f}\".format(\n            iteration, train_loss.loss.numpy()), end=\"\")\n        if iteration % 1000 == 0:\n            log_metrics(train_metrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef train_agent(n_iterations):\n    time_step = None\n    policy_state = agent.collect_policy.get_initial_state(tf_env.batch_size)\n    iterator = iter(dataset)\n\n    # Lists to store training information for plotting\n    iteration_values = []\n    loss_values = []\n\n    for iteration in range(n_iterations):\n        time_step, policy_state = collect_driver.run(time_step, policy_state)\n        trajectories, buffer_info = next(iterator)\n        train_loss = agent.train(trajectories)\n\n        # Collect data for plotting\n        iteration_values.append(iteration)\n        loss_values.append(train_loss.loss.numpy())\n\n        print(\"\\r{} loss: {:.5f}\".format(iteration, train_loss.loss.numpy()), end=\"\")\n        \n        if iteration % 1000 == 0:\n            log_metrics(train_metrics)\n\n    # Plot the training loss over iterations\n    plt.plot(iteration_values, loss_values, label='Training Loss')\n    plt.xlabel('Iteration')\n    plt.ylabel('Training Loss')\n    plt.title('Training Loss Over Iterations')\n    plt.legend()\n    plt.show()\n\n# Call the training function with the desired number of iterations\ntrain_iterations = 10000  # Replace with your desired number of iterations\ntrain_agent(train_iterations)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#el valor ideal para n_iterations es 1.000.000\nnum_iterations_ = 10000 #ojo entrenamiento\ntrain_agent(n_iterations=num_iterations_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization","metadata":{}},{"cell_type":"code","source":"def update_scene(num, frames, patch):\n    patch.set_data(frames[num])\n    return patch\n\ndef plot_animation(frames, repeat=False, interval=40):\n    fig = plt.figure()\n    patch = plt.imshow(frames[0])\n    plt.axis('off')\n    anim = animation.FuncAnimation(\n        fig, update_scene, fargs=(frames, patch),\n        frames=len(frames), repeat=repeat, interval=interval)\n    plt.close()\n    return anim","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frames = []\ndef save_frames(trajectory):\n    global frames\n    frames.append(tf_env.pyenv.envs[0].render(mode=\"rgb_array\"))\n\nprev_lives = tf_env.pyenv.envs[0].ale.lives()\ndef reset_and_fire_on_life_lost(trajectory):\n    global prev_lives\n    lives = tf_env.pyenv.envs[0].ale.lives()\n    if prev_lives != lives:\n        tf_env.reset()\n        tf_env.pyenv.envs[0].step(np.array(tf_env.pyenv.envs[0].action_space.sample()))\n        prev_lives = lives\n\nwatch_driver = DynamicStepDriver(\n    tf_env,\n    agent.policy,\n    observers=[save_frames, reset_and_fire_on_life_lost, ShowProgress(10000)],\n    num_steps=10000)\nfinal_time_step, final_policy_state = watch_driver.run()\n\nplot_animation(frames)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creando un gif\nimport PIL\nimport os\n\nimage_path = os.path.join(\"view1.gif\")\nframe_images = [PIL.Image.fromarray(frame) for frame in frames[:150]]\nframe_images[0].save(image_path, format='GIF',\n                     append_images=frame_images[1:],\n                     save_all=True,\n                     duration=300,\n                     loop=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%html\n<img src=\"view9e5.gif\" />","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References\n[Hands–On Machine Learning with Scikit–Learn and TensorFlow 2](https://www.amazon.com.br/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646)\n\n\n[Agents is a library for reinforcement learning in TensorFlow.\n](https://www.tensorflow.org/agents)\n\n\n[Introduction to TF-Agents : A library for Reinforcement Learning in TensorFlow](https://towardsdatascience.com/introduction-to-tf-agents-a-library-for-reinforcement-learning-in-tensorflow-68ab9add6ad6)\n\n\n[Train a Deep Q Network with TF-Agents](https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}